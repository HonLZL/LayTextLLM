{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "model_path = \"LayTextLLM/LayTextLLM-Zero\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code = True, padding_side = 'left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code = True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model generation setup\n",
    "generate_params = {\n",
    "    \"use_cache\": True,\n",
    "    \"do_sample\": False,\n",
    "    \"num_beams\": 1,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"min_new_tokens\": None,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.0,\n",
    "    \"length_penalty\": 1.0,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"temperature\": 1.0,\n",
    "    \"output_scores\": True,\n",
    "    \"output_hidden_states\": True,\n",
    "    \"output_attentions\": True,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"keyword\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_TOKEN = \"<unk>\"\n",
    "BOX_TOKEN_ID = 0\n",
    "INPUT_PROMPT_TEMPLATE = \"given document <document>{ocr}</document>, answer following question: {question} Please think step-by-step.\\n## answer:\"\n",
    "\n",
    "with open(\"datasets/funsd_test.json\", \"r\") as fin:\n",
    "    test_data = json.load(fin)\n",
    "\n",
    "print('==========num examples', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for idx,example in enumerate(tqdm(test_data[2:3])):\n",
    "\n",
    "        input_ids, input_polys = [], []\n",
    "        img_size = {}\n",
    "\n",
    "        texts = example['ocr']\n",
    "        polys = example['poly']\n",
    "        w, h = example['img_size']['w'], example['img_size']['h']\n",
    "        question = example['question']\n",
    "        answer = example['answer']\n",
    "        meta = example['metadata']\n",
    "\n",
    "        ## if ocr is empty, skip this example\n",
    "        if len(texts) == 0:\n",
    "            continue\n",
    "\n",
    "        ## prepare input text ids, and layout polys\n",
    "        for text, poly in zip(texts, polys):\n",
    "            input_ids += [BOX_TOKEN_ID]\n",
    "            text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "            input_ids += text_ids\n",
    "            text_poly = [poly[0]/w,poly[1]/h,poly[4]/w,poly[5]/h]\n",
    "            input_polys.append(text_poly)\n",
    "\n",
    "        # extract layout embeddings\n",
    "        input_polys = torch.as_tensor(input_polys).unsqueeze(0).to(device)\n",
    "\n",
    "        # extract text embeddings\n",
    "        # assign template to input texts\n",
    "        input_data = {\"ocr\": tokenizer.decode(input_ids), \"question\": question}\n",
    "        input_texts = INPUT_PROMPT_TEMPLATE.format(**input_data)\n",
    "\n",
    "        # extract text ids\n",
    "        input_ids = tokenizer.encode(input_texts, add_special_tokens=False)\n",
    "        input_ids = torch.as_tensor(input_ids).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "        # Forward pass with attention extraction\n",
    "        # outputs = model(\n",
    "        #     input_ids=input_ids,\n",
    "        #     laytout_input=input_polys,\n",
    "        #     attention_mask=attention_mask,\n",
    "        #     output_attentions=True\n",
    "        #     )\n",
    "        # attentions = outputs.attentions \n",
    "\n",
    "        model_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                laytout_input=input_polys,\n",
    "                attention_mask=attention_mask,\n",
    "                **generate_params\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, token_id in enumerate(model_output.sequences[0]):\n",
    "    print(idx, token_id, tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, token_id in enumerate(model_output.sequences[0][input_ids.size(1):]):\n",
    "    print(idx, token_id, tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output.attentions[2][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Let's assume you have attention maps for T tokens\n",
    "max_output_length = model_output.sequences.size(1)  # Number of tokens generated\n",
    "# max_source_length = input_ids.size(1)+1  # Maximum source sequence length\n",
    "\n",
    "full_attention_matrices = []\n",
    "\n",
    "for attention_score in model_output.attentions[len(input_ids):]:\n",
    "    # Load or extract the attention map for token t\n",
    "    # Average over heads and layers\n",
    "    # attention_map = torch.stack(attention_score).cpu().squeeze(1).squeeze(2).mean(dim=(0,1))  # mean of attention of all heads fo the all layers\n",
    "    attention_map = attention_score[0].cpu().squeeze(0).squeeze(1).max(dim=0)[0] # max of attention of all heads in the first layer\n",
    "    # attention_map = attention_score[-1].cpu().squeeze(0).squeeze(1).mean(dim=0) # mean of attention of all heads in the first layer\n",
    "\n",
    "\n",
    "    # Pad to max_source_length\n",
    "    padded_attention = np.pad(\n",
    "        attention_map,\n",
    "        (0, max_output_length - attention_map.shape[0]),\n",
    "        'constant',\n",
    "        constant_values=0\n",
    "    )\n",
    "\n",
    "\n",
    "    full_attention_matrices.append(padded_attention)\n",
    "\n",
    "# Stack to form the attention matrix\n",
    "full_attention_matrix = np.stack(full_attention_matrices, axis=0).T\n",
    "print(full_attention_matrix.shape)\n",
    "\n",
    "# Plot the attention matrix\n",
    "plt.figure(figsize=(100, 200))\n",
    "sns.heatmap(full_attention_matrix, cmap='viridis', cbar_kws={\"shrink\": 0.2})\n",
    "cbar = plt.gcf().axes[-1]  # Get the color bar axis\n",
    "cbar.tick_params(labelsize=30)  \n",
    "# Set tick gaps for both x and y axes\n",
    "# Set tick positions and labels\n",
    "source_tokens = [tokenizer.decode(input_id) for input_id in model_output.sequences[0]]\n",
    "target_tokens = [tokenizer.decode(input_id) for input_id in model_output.sequences[0][len(input_ids[0])+1:]]\n",
    "\n",
    "plt.yticks(ticks=np.arange(0, len(source_tokens)), labels=source_tokens, rotation=0, fontsize=30)\n",
    "plt.xticks(ticks=np.arange(0, len(target_tokens)), labels=target_tokens, rotation=90,  fontsize=30)\n",
    "plt.xlabel('Target Token', fontsize=30)\n",
    "plt.ylabel('Source Token', fontsize=30)\n",
    "plt.title('LayTextLLM Attention Map', fontsize=30)\n",
    "plt.savefig(\"LayTextLLM_Attention_Map.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_attention = model_output.attentions[0][0].squeeze(0).max(dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Let's assume you have attention maps for T tokens\n",
    "max_output_length = model_output.sequences.size(1)  # Number of tokens generated\n",
    "# max_source_length = input_ids.size(1)+1  # Maximum source sequence length\n",
    "\n",
    "attention_matrices = []\n",
    "\n",
    "for attention_score in model_output.attentions[-20:]:\n",
    "    # Load or extract the attention map for token t\n",
    "    # Average over heads and layers\n",
    "    # attention_map = torch.stack(attention_score).cpu().squeeze(1).squeeze(2).mean(dim=(0,1))  # mean of attention of all heads fo the all layers\n",
    "    attention_map = attention_score[-1].cpu().squeeze(0).squeeze(1).max(dim=0)[0][90:105] # max of attention of all heads in the first layer\n",
    "    # attention_map = attention_score[-1].cpu().squeeze(0).squeeze(1).mean(dim=0) # mean of attention of all heads in the first layer\n",
    "\n",
    "\n",
    "    # Pad to max_source_length\n",
    "    # padded_attention = np.pad(\n",
    "    #     attention_map,\n",
    "    #     (0, max_output_length - attention_map.shape[0]),\n",
    "    #     'constant',\n",
    "    #     constant_values=0\n",
    "    # )\n",
    "\n",
    "\n",
    "    attention_matrices.append(attention_map)\n",
    "\n",
    "# Stack to form the attention matrix\n",
    "attention_matrix = np.stack(attention_matrices, axis=0)\n",
    "\n",
    "question = \"What is the quantity of - TICKET CP? ...\"\n",
    "question_tokens = [tokenizer.decode(token_id) for token_id in tokenizer.encode(question, add_special_tokens=False)]\n",
    "\n",
    "attention_matrix[:len(question_tokens)] = input_attention[136:149, 90:105].cpu()\n",
    "print(attention_matrix.shape)\n",
    "\n",
    "# Plot the attention matrix\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(attention_matrix, cmap='viridis', cbar_kws={\"shrink\": 0.2})\n",
    "cbar = plt.gcf().axes[-1]  # Get the color bar axis\n",
    "cbar.tick_params(labelsize=30)  \n",
    "# Set tick gaps for both x and y axes\n",
    "# Set tick positions and labels\n",
    "source_tokens = [tokenizer.decode(input_id) for input_id in model_output.sequences[0]][90:105]\n",
    "\n",
    "target_tokens = [tokenizer.decode(input_id) for input_id in model_output.sequences[0][-21:-1]]\n",
    "target_tokens[:len(question_tokens)] = question_tokens\n",
    "\n",
    "plt.yticks(ticks=np.arange(0, len(target_tokens)), labels=target_tokens, rotation=0, fontsize=30)\n",
    "plt.xticks(ticks=np.arange(0, len(source_tokens)), labels=source_tokens, rotation=90,  fontsize=30)\n",
    "plt.xlabel('Source Token', fontsize=30)\n",
    "plt.ylabel('Target Token', fontsize=30)\n",
    "plt.title('LayTextLLM Attention Map', fontsize=30)\n",
    "plt.savefig(\"LayTextLLM_Attention_Map.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_attention[136:148].cpu().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_attention_matrix[:][159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_attention_score = []\n",
    "coordinate_idx = 0\n",
    "## get the attantion score of each coordinate\n",
    "for token_idx,token_id in enumerate(input_ids[0]):\n",
    "    if token_id == 0: # placeholder of layout embedding\n",
    "        vis_attention_score.append(full_attention_matrix[token_idx][159])\n",
    "        coordinate_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image_path = 'cord_example.jpg'  # Replace with your image path\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Example bounding box data [x, y, width, height, score]\n",
    "scores = vis_attention_score\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(image)\n",
    "\n",
    "# Define a colormap to map score to color\n",
    "cmap = plt.get_cmap('Reds')\n",
    "norm = mpl.colors.Normalize(vmin=min(scores), vmax=max(scores))\n",
    "bounding_boxes = polys\n",
    "# Plot each bounding box\n",
    "for i, box in enumerate(bounding_boxes):\n",
    "    x, y, w, h = box[0], box[1], box[4]-box[0], box[5]-box[1]\n",
    "    score = scores[i]\n",
    "    \n",
    "    # Get color based on the score\n",
    "    color = cmap(norm(score))  # Score between 0 and 1, maps to the colormap\n",
    "    \n",
    "    # Create a rectangle patch\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor=color, facecolor='none')\n",
    "    \n",
    "    # Add the rectangle to the plot\n",
    "    ax.add_patch(rect)\n",
    "# Create a ScalarMappable for the color bar\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])  # You have to set a dummy array for the color bar\n",
    "\n",
    "# Add color bar with legend\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Score')\n",
    "# Display the plot\n",
    "print(\"Figure size:\", fig.get_size_inches())\n",
    "plt.savefig(\"cord_example.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids = model_output.sequences[0]\n",
    "output_str = tokenizer.decode(output_ids, skip_special_tokens=False)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = [142, 438, 635, 453]\n",
    "x_min, y_min, x_max, y_max = area[0]/1000,  area[1]/1000,  area[2]/1000,  area[3]/1000\n",
    "width, height = w, h\n",
    "abs_x_min, abs_y_min, abs_x_max, abs_y_max = x_min*width, y_min*height, x_max*width, y_max*height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# Open the black-and-white image (grayscale mode 'L')\n",
    "image_path = '82092117.png'  # Replace with your image path\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Convert the grayscale image to RGB to allow for color drawing\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "# Create a drawing object\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "# Define the bounding box coordinates (x1, y1, x2, y2)\n",
    "bounding_box = (abs_x_min, abs_y_min, abs_x_max, abs_y_max)  # Replace with your bounding box coordinates\n",
    "\n",
    "# Draw the bounding box (outline with color and width)\n",
    "draw.rectangle(bounding_box, outline='red', width=3)\n",
    "\n",
    "# Show the image\n",
    "image.show()\n",
    "\n",
    "# Optionally save the image with the bounding box\n",
    "output_path = 'funsd_with_bbx.pdf'\n",
    "image.save(output_path, format=\"pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_x_min, abs_y_min, abs_x_max, abs_y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
